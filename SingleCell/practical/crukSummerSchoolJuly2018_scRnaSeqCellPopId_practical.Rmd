First, set some variables:

```{r setup}
knitr::opts_chunk$set(echo = TRUE)
options(stringsAsFactors = FALSE)
set.seed(123) # for reproducibility
```

# Identification of cell populations

In part 1 we gathered the data, aligned reads, checked quality, and normalised read counts for library size. We will now identify genes to focus on, use visualisation to explore the data, cluster cells by their expression profile and identify genes that best characterise these cell populations.  

<img src="images/Andrews2017_Fig1.png" style="width: 75%; margin: 0 6em;"/>

See https://f1000research.com/articles/5-2122/v2
See https://hemberg-lab.github.io/scRNA.seq.course/biological-analysis.html
See https://master.bioconductor.org/packages/release/workflows/vignettes/simpleSingleCell

## Dimension reduction

In a sigle cell RNA-seq (scRNASeq) data set, each cell is described by the expression level of thoushands of genes.

The total number of genes measured is referred to as dimensionality. Each gene measured is one dimension in the space characterising the data set. Many genes will little vary across cells and thus be uninformative when comparing cells. Also, because some genes will have correlated expression patterns, some information is redundant. Moreover, we can represent data in 3 dimensions, not more. So reducing the number of useful dimensions is necessary.

### Principal Component Analysis


#### Description

The data set: a matrix with one row per sample and one variable per column.  Here samples are cells and each variable is the normalised read count for a given gene.
The space: each cell is associated to a point in a multi-dimensional space where each gene is a dimension.
The aim: find a new set of variables defining a space with fewer dimensions while losing as little information as possible.

Out of a set of variables (read counts), PCA defines new variables called Principal Components (PCs) that best capture the variability observed amongst samples (cells).

The number of variables does not change. Only the fraction of variance captured by each variable differs.
The first PC explains the highest proportion of variance possible (bound by characteristic of PCA).
The second PC explains the highest proportion of variance not explained by the first PC.
PCs each explain a decreasing amount of variance not explained by the previous ones.
Each PC is a dimenion in the new space.

The total amount of variance explained by the first few PCs is usually such that excluding remaining PCs, ie dimensions, losses little information. The stronger the correlation between the initial variables, the stronger the reduction in dimensionality. PCs to keep can be chosen as those capturing at least as much as the average variance per initial variables or using a scree plot, see below.

PCs are linear combinations of the initial variables. PCs represent the same amount of information as the initial set and enable its restoration. The data is not altered. We only look at it in a different way.

If they were centered and scaled, that should be 1.

About the mapping function from the old to the new space:
- it is linear
- it is inverse, to restore the original space
- it relies on orthogonal PCs so that the total variance remains the same.

Two transformations of the data are necessary:
- center the data so that the sample mean for each column is 0 so the covariance matrix of the intial matrix takes a simple form
- scale variance to 1, ie standardize to avoid PCA loading on variable with large variance.

<!--
Link between variance and Pythagore but need schema.
variance is proportional to the sum of square of the distance between the cloud centroid and each point. For a given point, the square of the distance between the centroid and the point is the sum of the distance between the point and its (orthogonal) projection on a plane and the square of the distance the projection and the centroid. As the distance between the point and the centroid does not change (total variance remains the same), minimising the distance between points and their projection maximises the distance between their projection and the centroid.
-->

#### example

Based on https://liorpachter.wordpress.com/2014/05/26/what-is-principal-component-analysis/

```{r}
library(ggplot2)
```

Let's make and plot a data set.

```{r}
set.seed(123)            #sets the seed for random number generation.
 x <- 1:100              #creates a vector x with numbers from 1 to 100
 ex <- rnorm(100, 0, 30) #100 normally distributed rand. nos. w/ mean=0, s.d.=30
 ey <- rnorm(100, 0, 30) # " " 
 y <- 30 + 2 * x         #sets y to be a vector that is a linear function of x
 x_obs <- x + ex         #adds "noise" to x
 y_obs <- y + ey         #adds "noise" to y
 P <- cbind(x_obs,y_obs) #places points in matrix
 plot(P,asp=1,col=1) #plot points
 points(mean(x_obs),mean(y_obs),col=3, pch=19) #show center
```

Center the data and compute covariance matrix.

```{r}
M <- cbind(x_obs - mean(x_obs), y_obs - mean(y_obs)) #centered matrix
MCov <- cov(M)          #creates covariance matrix
# no scaling? No need because the Normal distriution used to make data set have the same variance
```

Compute the principal axes, ie eigenvectors and corresponding eigenvalues.

An eigenvector is a direction and an eigenvalue is a number measuring the spread of the data in that direction. The eigenvector with the highest eigenvalue is the first principal component.

The eigenvectors of the covariance matrix provide the principal axes, and the eigenvalues quantify the fraction of variance explained in each component.

```{r}
eigenValues <- eigen(MCov)$values       #compute eigenvalues
eigenVectors <- eigen(MCov)$vectors     #compute eigenvectors

# or use 'singular value decomposition' of the matrix
d <- svd(M)$d          #the singular values
v <- svd(M)$v          #the right singular vectors
```

Let's plot the principal axes.

First PC:

```{r, echo=FALSE}
#pcaExample()
# PC 1:
 plot(P,asp=1,col=1) #plot points
 points(mean(x_obs),mean(y_obs),col=3, pch=19) #show center
lines(x_obs,eigenVectors[2,1]/eigenVectors[1,1]*M[x]+mean(y_obs),col=8)
```

Second PC:

```{r}
 plot(P,asp=1,col=1) #plot points
 points(mean(x_obs),mean(y_obs),col=3, pch=19) #show center
# PC 1:
lines(x_obs,eigenVectors[2,1]/eigenVectors[1,1]*M[x]+mean(y_obs),col=8)
# PC 2:
lines(x_obs,eigenVectors[2,2]/eigenVectors[1,2]*M[x]+mean(y_obs),col=8)
```

Add the projections of the points onto the first principal component:

```{r}
 plot(P,asp=1,col=1) #plot points
 points(mean(x_obs),mean(y_obs),col=3, pch=19) #show center
# PC 1:
lines(x_obs,eigenVectors[2,1]/eigenVectors[1,1]*M[x]+mean(y_obs),col=8)
# PC 2:
lines(x_obs,eigenVectors[2,2]/eigenVectors[1,2]*M[x]+mean(y_obs),col=8)
# add projecions:
trans <- (M%*%v[,1])%*%v[,1] #compute projections of points
P_proj <- scale(trans, center=-cbind(mean(x_obs),mean(y_obs)), scale=FALSE) 
points(P_proj, col=4,pch=19,cex=0.5) #plot projections
segments(x_obs,y_obs,P_proj[,1],P_proj[,2],col=4,lty=2) #connect to points
```

Could use prcomp().

```{r}
pca_res <- prcomp(M)
```

```{r}
df_pc <- data.frame(pca_res$x)
g <- ggplot(df_pc, aes(PC1, PC2)) + 
  geom_point(size=2) +   # draw points
  labs(title="PCA", 
       subtitle="With principal components PC1 and PC2 as X and Y axis") + 
  coord_cartesian(xlim = 1.2 * c(min(df_pc$PC1), max(df_pc$PC1)), 
                  ylim = 1.2 * c(min(df_pc$PC2), max(df_pc$PC2)))
g <- g + geom_hline(yintercept=0)
g <- g + geom_vline(xintercept=0)
g
```


```{r}
# ggfortify
library(ggfortify)
g <- autoplot(pca_res)
g <- g + geom_hline(yintercept=0)
g <- g + geom_vline(xintercept=0)
g
```

Going from 2D to 3D:

<img src="images/hemberg_pca.png" style="width: 75%; margin: 0 6em;"/>

Now let's analyse our data set.

## Load packages

```{r packages, results='hide', message=FALSE, warning=FALSE}
library(scater) # for QC and plots
library(scran) # for normalisation
library(dynamicTreeCut)
library(cluster)
library(broom)
library(tibble)
library(dplyr)
library(tidyr)
library(purrr)
library(pheatmap)
library(RColorBrewer)
library(viridis)
```

```{r}
fontsize <- theme(axis.text=element_text(size=12), axis.title=element_text(size=16))
```

## Load normalised counts

The R object keeping the normalised counts obtained at the end of part 1 was written to a file for you: Tcells_SCE.Rds. Let's load this file.

```{r set_var}
# dir
inpDir <- "/home/participant/Course_Materials/SinglecellToUse/HumanBreastTCells"
dataSubDir <- "GRCh38"
```

```{r load_normalised_counts}
# file
rObjFile <- "Tcells_SCE.Rds"

# check dir exist:
if(! dir.exists(inpDir))
{ stop(sprintf("ERROR: Cannot find dir inpDir '%s'", inpDir)) }
if(! dir.exists(file.path(inpDir, dataSubDir)))
{ stop(sprintf("ERROR: Cannot find dir dataSubDir '%s'", file.path(inpDir, dataSubDir))) }

# check file exists:
tmpFileName <- file.path(inpDir, dataSubDir, rObjFile)
if(! file.exists(tmpFileName))
{ stop(sprintf("ERROR: Cannot find dir tmpFileName '%s'", tmpFileName)) }

# load file:
# Remember name of object saved in the file, or make up a new one
nz.sce <- readRDS(tmpFileName)

# check:
nz.sce
# features data:
head(rowData(nz.sce))
#any(duplicated(rowData(nz.sce)$ensembl_gene_id))
# some function(s) used below complain about 'strand' already being used in row data,
# so rename that column now:
colnames(rowData(nz.sce))[colnames(rowData(nz.sce)) == "strand"] <- "strandNum"

# have sample name Tils20 for Tils20_1 and Tils20_2
tmpBatch <- gsub("_[12]", "", colData(nz.sce)$Sample)
colData(nz.sce)$Sample2 <- tmpBatch
```

## Data exploration with dimensionality reduction

### PCA

Perform PCA, keep outcome in new object.

Display cells on a plot for the first 2 PCs, coloring by 'Sample' and setting size to match 'total_features'. Proximity of cells reflects similarity of their expression profiles.

```{r}
nz.sce <- runPCA(nz.sce, ncomponents = 10, method = "irlba")
g <- plotPCA(nz.sce)
#sce3 <- runPCA(nz.sce, ncomponents = 10, method = "prcomp")
#plotPCA(sce3)
```

```{r, include=TRUE}
g <- plotPCA(nz.sce,
    colour_by = "Sample",
    size_by = "total_features"
)         
g
rObjFile <- "Tcells_SCE_comb_plotQCA_1.png"
tmpFileName <- file.path(inpDir, dataSubDir, rObjFile)
ggsave(plot=g, filename = tmpFileName)
```

Any observation?

One can also split the plot, say by sample.

```{r}
g <- g +  facet_grid(nz.sce$Sample ~ .)
g
rObjFile <- "Tcells_SCE_comb_plotQCA_2.png"
tmpFileName <- file.path(inpDir, dataSubDir, rObjFile)
ggsave(plot=g, filename = tmpFileName)
```

Several PCs at once, using plotReducedDim():

```{r}
plotReducedDim(nz.sce, use_dimred="PCA", ncomponents=3, 
    colour_by="Sample",
    size_by="total_features") + fontsize
```

### t-SNE

PCA represents relationships in the high-dimensional space linearly, while t-SNE allows non-linear relationships and thus usually separates cells from diverse populations better.

t-SNE stands for "T-distributed stochastic neighbor embedding". It is a stochastic method to visualise large high dimensional datasets by preserving local structure amongst cells. 

Two parameters matter:

- perplexity, to indicate the balance between local and global structure,
- stochasticity; running the analysis will produce a different map every time, unless the seed is set.

See https://distill.pub/2016/misread-tsne/.

#### perplexity

Compute t-SNE with default perplexity, ie 50.

```{r}
run_args <- list(use_dimred="PCA") # rand_seed=100

# runTSNE defult perpexity if min(50, floor(ncol(object)/5))

nz.sce <- runTSNE(nz.sce, use_dimred="PCA", perplexity=50)

tsne50 <- plotTSNE(nz.sce, run_args=c(run_args, perplexity=50),
    colour_by="Sample", size_by="total_features") + fontsize + ggtitle("Perplexity = 50")
```

Plot t-SNE:

```{r}
tsne50 <- plotTSNE(nz.sce, colour_by="Sample", size_by="total_features") + fontsize + ggtitle("Perplexity = 50")
tsne50
```

Split by sample:

```{r}
g <- tsne50 + facet_grid(. ~ nz.sce$Sample2)
g
```

Compute t-SNE for several perplexity values: 

```{r}
tsne5 <- plotTSNE(nz.sce, run_args=c(run_args, perplexity=5),
    colour_by="Sample", size_by="total_features") + fontsize + ggtitle("Perplexity = 5")

#tsne10 <- plotTSNE(nz.sce, run_args=c(run_args, perplexity=10),
#    colour_by="Sample", size_by="total_features") + fontsize + ggtitle("Perplexity = 10")

tsne30 <- plotTSNE(nz.sce, run_args=c(run_args, perplexity=30),
    colour_by="Sample", size_by="total_features") + fontsize + ggtitle("Perplexity = 30")

tsne100 <- plotTSNE(nz.sce, run_args=c(run_args, perplexity=100),
    colour_by="Sample", size_by="total_features") + fontsize + ggtitle("Perplexity = 100")

#tsne200 <- plotTSNE(nz.sce, run_args=c(run_args, perplexity=200),
#    colour_by="Sample") + fontsize + ggtitle("Perplexity = 200")
```

```{r}
#multiplot(tsne5, tsne10, tsne20, tsne50, tsne100, tsne200, cols=2)
multiplot(tsne5, tsne30, tsne50, tsne100, cols=2)
```

Challenge: t-SNE is a stochastic method. Change the seed with set.seed(), make and plot t-SNE. Try that a few times.

### Other methods

Several other dimensionality reduction techniques could also be used, e.g., multidimensional scaling, diffusion maps.

<!--
Challenge: Use diffusion map place cells along a continuous trajectory and are suited for visualizing graduated processes like differentiation (see plotDiffusionMap) 
-->
<!--(Angerer et al. 2016)--> 

```{r, include=FALSE}
if(FALSE)
{
sce2 <- runDiffusionMap(nz.sce)
reducedDimNames(sce2)
head(reducedDim(sce2))
plotDiffusionMap(sce2, colour_by="Sample") # malformed factor
}
```

## PCA

<!-- play pca ? -->

### Correlation between PCs and the total number of features detected

The PCA plot above shows cells as symbols whose size depends on the total number of features or library size. It suggests there may be a correlation between PCs and these variables. Let's check:

```{r}
g <- plotQC(
    nz.sce,
    type = "find-pcs",
    exprs_values = "logcounts",
    variable = "total_features"
)
g
```

```{r}
rObjFile <- "Tcells_SCE_comb_plotQC_PC.png"
tmpFileName <- file.path(inpDir, dataSubDir, rObjFile)
ggsave(plot=g, filename = tmpFileName)
```

These plots show that PC2 and PC1 correlate with the number of detected genes. This correlation is often observed.

Challenge: Check correlation of PCs with library size. Was the outcome expected?

```{r}
g <- plotQC(
    nz.sce,
    type = "find-pcs",
    exprs_values = "logcounts",
    variable = "total_counts"
)
g
```

<!--
https://www.biorxiv.org/content/early/2015/12/27/025528
https://www.ncbi.nlm.nih.gov/pubmed/29121214
-->

## Feature selection

scRNASeq measures the expression of thousands of genes in each cell. The biological question asked in a study will most often relates to a fraction of these genes. <!-- difference between cell types, drivers of differentiation, espond to perturbation -->

Most high-throughput molecular data include variation created by the assay itself, not biology, i.e. technical noise. <!-- e.g. sampling during RNA capture and library preparation --> In scRNASeq, this technical noise will result in most genes being detected at different levels. This noise may hinder the detection of the biological signal.

Let's identify Highly Variables Genes (HVGs) with the aim to find those underlying the heterogeneity observed across cells.

### Modelling and removing technical noise

Some assays allow the inclusion of known molecules in a known amount covering a wide range, from low to high abundance: spike-ins. The techincal noise is assessed by comparing the amount of spike-ins used to the corresponding read counts obtained and in particular their variation across cells. <!-- mind assumption that spike-in actually work --> Technically, the variance is decomposed into the biolgical and technical components. 

UMI- based assays do not (yet?) allow spike-ins. But one can still identify HVGs with the highest biolgical component. Assuming that expression does not vary across cells for most genes, the total variance for these genes mainly reflect techincal noise. The latter can thus be assessed by fitting a trend to the variance in expression. The fitted value will be the estimate of the technical component.


Let's fit a trend to the variance, using trendVar(). 

```{r fit_trend_to_var}
var.fit <- trendVar(nz.sce, method="loess", use.spikes=FALSE, loess.args=list("span"=0.05)) 
#var.fit <- trendVar(nz.sce, method="loess", use.spikes=FALSE, loess.args=list("span"=0.1)) 
#var.fit <- trendVar(nz.sce, method="loess", use.spikes=FALSE, loess.args=list("span"=0.2)) 
#var.fit <- trendVar(nz.sce, method="loess", use.spikes=FALSE, loess.args=list("span"=0.4)) 
```

Plot variance against mean of expression (log scale) and the mean-dependent trend fitted to the variances of the (endogenous) genes: 

```{r plot_var_trend}
plot(var.fit$mean, var.fit$var)
curve(var.fit$trend(x), col="red", lwd=2, add=TRUE)
```

```{r save_var_trend_plot}
rObjFile <- "Tcells_SCE_comb_fit.png"
tmpFileName <- file.path(inpDir, dataSubDir, rObjFile)
png(tmpFileName)
plot(var.fit$mean, var.fit$total, pch=16, cex=0.6, xlab="Mean log-expression",  ylab="Variance of log-expression")
#points(var.out$mean[isSpike(sce)], var.out$total[isSpike(sce)], col="red", pch=16)
curve(var.fit$trend(x), col="dodgerblue", add=TRUE, lwd=2)
dev.off()
```

Decompose variance into technical and biological components:

```{r}
var.out <- decomposeVar(nz.sce, var.fit)
```

Choose some HGVs:

```{r}
o <- order(var.out$bio, decreasing=TRUE)
head(var.out[o,])
tail(var.out[o,])
chosen.genes.index <- o[1:20]
```

Show HGVs: 

```{r plot_var_trend_hgv}
plot(var.fit$mean, var.fit$var)
curve(var.fit$trend(x), col="red", lwd=2, add=TRUE)
points(var.fit$mean[chosen.genes.index], var.fit$var[chosen.genes.index], col="orange")
```

Show all 'HGVs':

```{r}
hvgBool <- var.out$bio > 0
table(hvgBool)
hvg.index <- which(hvgBool)
plot(var.fit$mean, var.fit$var)
curve(var.fit$trend(x), col="red", lwd=2, add=TRUE)
points(var.fit$mean[hvg.index], var.fit$var[hvg.index], col="orange")
```

<!--
Question: in experiments with spike-ins, the trend fitted would rely on their expression. In a sample with different cell types, how would you expect that trend to look?

Answer: the variances for spike-ins should be lower than the variances of the endogenous genes.
-->

```{r trendVar_batch_fit_and_plot, include=FALSE, echo=FALSE}
if(FALSE)
{
#batch <- rep(c("1", "2"), each=100)
alt.fit <- trendVar(nz.sce, method="loess", use.spikes=FALSE, loess.args=list("span"=0.05), block=colData(nz.sce)$Sample2)
plot(alt.fit$mean, alt.fit$var, col=as.numeric(as.factor(colData(nz.sce)$Sample2))+2)
curve(alt.fit$trend(x), col="red", lwd=2, add=TRUE)
alt.decomp <- decomposeVar(nz.sce, alt.fit)
}
```

```{r trendVar_batch_plot_save, include=FALSE, echo=FALSE}
if(FALSE)
{
rObjFile <- "Tcells_SCE_comb_fitWiBatch.png"
tmpFileName <- file.path(inpDir, dataSubDir, rObjFile)
png(tmpFileName)
plot(alt.fit$mean, alt.fit$total, pch=16, cex=0.6, xlab="Mean log-expression", 
    ylab="Variance of log-expression")
curve(alt.fit$trend(x), col="dodgerblue", add=TRUE, lwd=2)
dev.off()
}
```

<!--
Check ID of gene with very high variance
-->

```{r check_gene_with_high_variance, include=FALSE, echo=FALSE}
if(FALSE)
{
tmpInd <- which(var.out$total == max(var.out$total))
var(counts(nz.sce)[tmpInd,])
var(logcounts(nz.sce)[tmpInd,])
##boxplot(counts(nz.sce)[tmpInd,])
##boxplot(logcounts(nz.sce)[tmpInd,])
##hist(counts(nz.sce)[tmpInd,], n=50)
##hist(logcounts(nz.sce)[tmpInd,], n=50)
##table(counts(nz.sce)[tmpInd,] > 0) # 2797F and 3525 T
#tmpVec <- seq.int(tmpInd-3,tmpInd+3)
#boxplot(counts(sce)[tmpVec,])
require(dplyr)
rowData(nz.sce) %>% as.data.frame %>% filter(ensembl_gene_id == rownames(nz.sce)[tmpInd])
# ENSG00000271503 is CCL5
}
```

HVGs may be driven by outlier cells. So let's plot the  distribution of expression values for the genes with the largest biological components.

```{r}
# the count matrix rows are named with ensembl gene IDs. Let's label gene with their name instead:
# row indices of genes in rowData(nz.sce)
tmpInd <- (which(rowData(nz.sce)$ensembl_gene_id %in% rownames(var.out)[chosen.genes.index]))
# check:
rowData(nz.sce)[tmpInd,c("ensembl_gene_id","external_gene_name")]
# store names:
tmpName <- rowData(nz.sce)[tmpInd,"external_gene_name"]
# the gene name may not be known, so keep the ensembl gene ID in that case:
tmpName[tmpName==""] <- rowData(nz.sce)[tmpInd,"ensembl_gene_id"][tmpName==""]
tmpName[is.na(tmpName)] <- rowData(nz.sce)[tmpInd,"ensembl_gene_id"][is.na(tmpName)]
rm(tmpInd)
```

Show a violin plot for each gene, using plotExpression():

```{r}
g <- plotExpression(nz.sce, rownames(var.out)[chosen.genes.index], 
    alpha=0.05, jitter="jitter") + fontsize
g <- g + scale_x_discrete(breaks=rownames(var.out)[chosen.genes.index],
        labels=tmpName)
g
rObjFile <- "Tcells_nz.sce_comb_plotExpress_2.png"
tmpFileName <- file.path(inpDir, dataSubDir, rObjFile)
ggsave(plot=g, filename = tmpFileName)
```

Challenge: Show a violin plots for the 20 genes with the lowest biological component. How do the profiles for these genes comapare to the those for HVGs chosen above?

```{r}
if(FALSE)
{
chosen.genes.index.tmp <- order(var.out$bio, decreasing=FALSE)[1:20]
tmpInd <- (which(rowData(nz.sce)$ensembl_gene_id %in% rownames(var.out)[chosen.genes.index.tmp]))
# check:
rowData(nz.sce)[tmpInd,c("ensembl_gene_id","external_gene_name")]
# store names:
tmpName <- rowData(nz.sce)[tmpInd,"external_gene_name"]
# the gene name may not be known, so keep the ensembl gene ID in that case:
tmpName[tmpName==""] <- rowData(nz.sce)[tmpInd,"ensembl_gene_id"][tmpName==""]
tmpName[is.na(tmpName)] <- rowData(nz.sce)[tmpInd,"ensembl_gene_id"][is.na(tmpName)]
rm(tmpInd)
g <- plotExpression(nz.sce, rownames(var.out)[chosen.genes.index.tmp], 
    alpha=0.05, jitter="jitter") + fontsize
#g
#rObjFile <- "Tcells_nz.sce_comb_plotExpress_1.png"
#tmpFileName <- file.path(inpDir, dataSubDir, rObjFile)
#ggsave(plot=g, filename = tmpFileName)
g <- g + scale_x_discrete(breaks=rownames(var.out)[chosen.genes.index.tmp],
        labels=tmpName)
g
rm(chosen.genes.index.tmp)
}
```

## Denoising expression values using PCA

Aim: use the trend fitted above to identify PCs linked to biology

Assumption: biology drives most of the variance hence should be captured by the first PCs, while technical noise affect each gene independently, hence captured by later PCs.

Logic: Compute the sum of the technical component across genes used in the PCA, use it as the amount of variance not related to biology and that we could/should therefore ignore. Later PCs are therefore remove until the amount of variance they account for matches that corresponding to the technical component. 

```{r denoisePCA, include=TRUE}
nz.sce.copy <- nz.sce # because nz.sce already keeps PCA.

rObjFile <- "Tcells_SCE_comb_denoisePCA.Rds"
tmpFileName <- file.path(inpDir, dataSubDir, rObjFile)
if(file.exists(tmpFileName))
{
  readRDS(tmpFileName)
} else
{
nz.sce <- denoisePCA(nz.sce, technical=var.fit$trend, assay.type="logcounts", approximate=TRUE)
assayNames(nz.sce)
dim(reducedDim(nz.sce, "PCA")) 
assayNames(nz.sce)
saveRDS(nz.sce, file=tmpFileName)
}
```

```{r}
plotReducedDim(nz.sce, use_dimred="PCA", ncomponents=3, 
    colour_by="Sample",
    size_by="total_features") + fontsize
```

### visualise expression patterns of some HVGs

on PCA plot:

```{r}
pca1 <- plotReducedDim(nz.sce, use_dimred="PCA", colour_by=rowData(nz.sce)[chosen.genes.index[1],"ensembl_gene_id"]) + fontsize + coord_fixed()
pca2 <- plotReducedDim(nz.sce, use_dimred="PCA", colour_by=rowData(nz.sce)[chosen.genes.index[2],"ensembl_gene_id"]) + fontsize + coord_fixed()
multiplot(pca1, pca2, cols=2)
multiplot(pca1+ facet_grid(. ~ nz.sce$Sample2), pca2+ facet_grid(. ~ nz.sce$Sample2), cols=2)
```

on t-SNE plot:

```{r}
tsne1 <- plotTSNE(nz.sce, colour_by=rowData(nz.sce)[chosen.genes.index[1],"ensembl_gene_id"]) + fontsize
tsne2 <- plotTSNE(nz.sce, colour_by=rowData(nz.sce)[chosen.genes.index[2],"ensembl_gene_id"]) + fontsize
multiplot(tsne1, tsne2, cols=2)
multiplot(tsne1 + facet_grid(. ~ nz.sce$Sample2), tsne2 + facet_grid(. ~ nz.sce$Sample2), cols=2)
```

## Clustering cells into putative subpopulations

<!--
See https://hemberg-lab.github.io/scRNA.seq.course/index.html for three types of clustering.
See https://www.ncbi.nlm.nih.gov/pubmed/27303057 for review
-->

### Defining cell clusters from expression data

See [clustering methods](https://hemberg-lab.github.io/scRNA.seq.course/biological-analysis.html##clustering-methods) on the Hemberg lab material.

We will use the denoised log-expression values to cluster cells.

#### hierarchical clustering

Here we'll use hierarchical clustering on the Euclidean distances between cells, using Ward D2s criterion to minimize the total variance within each cluster.

This yields a dendrogram that groups together cells with similar expression patterns across the chosen genes.

##### clustering

Compute tree:

```{r}
# get PCs
pcs <- reducedDim(nz.sce, "PCA")
# compute distance:
my.dist <- dist(pcs)
# derive tree:
my.tree <- hclust(my.dist, method="ward.D2")
```

Show tree:

```{r}
plot(my.tree, labels = FALSE)
```

Clusters are identified in the dendrogram using a dynamic tree cut <!-- (Langfelder, Zhang, and Horvath 2008) --> We also set minClusterSize to a lower value than the default of 20.

```{r}
library(dynamicTreeCut)
my.clusters <- unname(cutreeDynamic(my.tree, distM=as.matrix(my.dist), minClusterSize=20, verbose=0))
```

Let's count cells for each cluster and each sample.

```{r}
table(my.clusters, nz.sce$Sample)
```

Clusters mostly include cell from one sample or the other. This suggests that the two samples differ, and/or the presence of batch effect.

Let's show cluster assignemnts on the t-SNE. Cells in the same area are not all assigned to the same cluster.

```{r}
nz.sce$cluster <- factor(my.clusters)
tsne1 <- plotTSNE(nz.sce, colour_by=rowData(nz.sce)[chosen.genes.index[1],"ensembl_gene_id"]) + fontsize

g <- plotTSNE(nz.sce, colour_by = "cluster", size_by = "total_features")
g
g <- g + facet_grid(. ~ nz.sce$Sample)
g
```

##### separatedness

The congruence of cluster may be assessed by computing the sillhouette for each cell. The larger the value the closer the cell to cells in its cluster than to cells in other clusters. Cells closer to cells in other clusters have negative value.
Good cluster separation is indicated by clusters whose cells have large silhouette values.

Compute silhouette: 

```{r}
library(cluster)
sil <- silhouette(my.clusters, dist = my.dist)
```

Plot silhouettes with one color per cluster and cells with a negative silhouette with the color of their closest cluster. Add the average silhouette for each cluster and all cells. 


```{r}
clust.col <- scater:::.get_palette("tableau10medium") # hidden scater colours
sil.cols <- clust.col[ifelse(sil[,3] > 0, sil[,1], sil[,2])]
sil.cols <- sil.cols[order(-sil[,1], sil[,3])]
plot(sil, main = paste(length(unique(my.clusters)), "clusters"), 
    border=sil.cols, col=sil.cols, do.col.sort=FALSE) 
```

The plot shows many cells with negative silhoutette indicating too many clusters were defined. The method and parameters used defined with properties that may not fit the data set, eg clustere with the same diameter.

#### k-means

This approach assumes a pre-determined number of round equally-sized clusters.

The dendogram built above suggests there may be 6 populations.

Let's define 6 clusters.

```{r}
kclust <- kmeans(pcs, centers=6)

# compute silhouette
require("cluster")
sil <- silhouette(kclust$cluster, dist(pcs))

# plot silhouette:
clust.col <- scater:::.get_palette("tableau10medium") # hidden scater colours
sil.cols <- clust.col[ifelse(sil[,3] > 0, sil[,1], sil[,2])]
sil.cols <- sil.cols[order(-sil[,1], sil[,3])]
plot(sil, main = paste(length(unique(kclust$cluster)), "clusters"), 
    border=sil.cols, col=sil.cols, do.col.sort=FALSE) 

tSneCoord <- as.data.frame(reducedDim(nz.sce, "TSNE"))
colnames(tSneCoord) <- c("x", "y")
p2 <- ggplot(tSneCoord, aes(x, y)) +
	geom_point(aes(color = as.factor(kclust$cluster)))
p2 + facet_wrap(~ nz.sce$Sample2)
```

To find the most appropriate number of clusters, one performs the analysis for a series of k, computes a measure of fit of cluster defined: the within clusteer sum-of-square. This value descreases as k increases, by an amount that decreases with k. Choose k a the inflexion point of the curve. 

```{r}
library(broom)
require(tibble)
require(dplyr)
require(tidyr)
library(purrr)
points <- as.tibble(pcs)
augment(kclust, points)

kclusts <- tibble(k = 1:9) %>%
  mutate(
    kclust = map(k, ~kmeans(points, .x)),
    tidied = map(kclust, tidy),
    glanced = map(kclust, glance),
    augmented = map(kclust, augment, points)
  )

kclusts

clusters <- kclusts %>%
  unnest(tidied)

assignments <- kclusts %>% 
  unnest(augmented)

clusterings <- kclusts %>%
  unnest(glanced, .drop = TRUE)
```

Plot the total wihtin cluster sum-of-squares and decide on k.

```{r}
ggplot(clusterings, aes(k, tot.withinss)) +
  geom_line()
```

Check silhouette for a k of 5.

Copy the cluster assignment to the SCE object.

```{r}
df <- as.data.frame(assignments)
nz.sce$kmeans5 <- as.numeric(df[df$k == 5, ".cluster"])
```

Check silhouette.

```{r}
library(cluster)
clust.col <- scater:::.get_palette("tableau10medium") # hidden scater colours
sil <- silhouette(nz.sce$kmeans5, dist = my.dist)
sil.cols <- clust.col[ifelse(sil[,3] > 0, sil[,1], sil[,2])]
sil.cols <- sil.cols[order(-sil[,1], sil[,3])]
plot(sil, main = paste(length(unique(nz.sce$kmeans5)), "clusters"), 
    border=sil.cols, col=sil.cols, do.col.sort=FALSE) 
```

#### graph-based clustering

Let's build a shared nearest-neighbour graph using cells as nodes, then perform community-based clustering.

Build graph, define clusters, check membership across samples, show membership on t-SNE.

```{r}
#buildSNNGraph(x, ..., subset.row=NULL, assay.type="logcounts",  get.spikes=FALSE, use.dimred=NULL)
snn.gr <- buildSNNGraph(nz.sce, use.dimred="PCA")
cluster.out <- igraph::cluster_walktrap(snn.gr)
my.clusters <- cluster.out$membership
table(my.clusters)

nz.sce$cluster <- factor(my.clusters)
plotTSNE(nz.sce, colour_by="cluster") + fontsize
```

Check silhouette:

```{r}
require(cluster)
clust.col <- scater:::.get_palette("tableau10medium") # hidden scater colours
sil <- silhouette(my.clusters, dist(reducedDim(nz.sce, "PCA")))
sil.cols <- clust.col[ifelse(sil[,3] > 0, sil[,1], sil[,2])]
sil.cols <- sil.cols[order(-sil[,1], sil[,3])]
plot(sil, main = paste(length(unique(my.clusters)), "clusters"), 
    border=sil.cols, col=sil.cols, do.col.sort=FALSE) 
```

Compute modularity. The closer to 1 the better.

```{r, include = FALSE}
igraph::modularity(cluster.out)
```

```{r, include = FALSE}
mod.out <- clusterModularity(snn.gr, my.clusters, get.values=TRUE)
ratio <- mod.out$observed/mod.out$expected
lratio <- log10(ratio + 1)

library(pheatmap)
pheatmap(lratio, cluster_rows=FALSE, cluster_cols=FALSE, 
    color=colorRampPalette(c("white", "blue"))(100))
```

Show similarity between clusters on a network. 

```{r}
cluster.gr <- igraph::graph_from_adjacency_matrix(ratio, 
    mode="undirected", weighted=TRUE, diag=FALSE)
plot(cluster.gr, edge.width=igraph::E(cluster.gr)$weight*10)  
```

### Detecting genes differentially expressed between clusters

#### Differential expression analysis

Let's identify genes for each cluster whose expression differ to that of other clusters, using findMarkers(). It fits a linear model to the log-expression values for each gene using limma <!-- (Ritchie et al. 2015) --> and allows testing for differential expression in each cluster compared to the others while accounting for known, uninteresting factors.
 
```{r}
markers <- findMarkers(nz.sce, my.clusters)
```

Results are compiled in a single table per cluster taht stores the outcome of comparisons against other cluster. One can then select differentially expressed genes from each pairwise comparison between clusters.

Let's define a set of genes for cluster 1 by selecting the top 10 genes of each comparison, and check test output, eg adjusted p-values and log-fold changes.

```{r}
marker.set <- markers[["1"]]
head(marker.set, 10)

# add gene annotation:
tmpDf <- marker.set
tmpDf$ensembl_gene_id <- rownames(tmpDf)
tmpDf2 <- base::merge(tmpDf, rowData(nz.sce), by="ensembl_gene_id", all.x=TRUE, all.y=F, sort=F)
```

```{r}
rObjFile <- "Tcells_nz.sce_comb_clu1_deg.tsv"
tmpFileName <- file.path(inpDir, dataSubDir, rObjFile)
write.table(tmpDf2, file=tmpFileName, sep="\t", quote=FALSE, row.names=FALSE)
```

Gene set enrichment analyses learnt earlier today may be used to charaterise clusters further. 

#### heatmap

As for bulk RNA, differences in expression profiles of the top genes can be visualised with a heatmap. 

```{r}

library(pheatmap)
library(RColorBrewer)
library(viridis)

# select some top genes:
#top.markers <- rownames(marker.set)[marker.set$Top <= 100]
top.markers <- rownames(marker.set)[marker.set$Top <= 10]

# have matric to display
tmpData <- logcounts(nz.sce)[top.markers,]
tmpCellNames <- paste(colData(nz.sce)$Sample, colData(nz.sce)$Barcode, sep="_")
colnames(tmpData) <- tmpCellNames # colData(nz.sce)$Barcode                    

# columns annotation with cell name:
mat_col <- data.frame(cluster = nz.sce$cluster, sample = nz.sce$Sample)
rownames(mat_col) <- colnames(tmpData)
rownames(mat_col) <- tmpCellNames # colData(nz.sce)$Barcode

colourCount = length(unique(nz.sce$cluster))
getPalette = colorRampPalette(brewer.pal(9, "Set1"))

mat_colors <- list(group = getPalette(colourCount))
names(mat_colors$group) <- unique(nz.sce$cluster)

pheatmap(tmpData,
           border_color      = NA,
  show_colnames     = FALSE,
  show_rownames     = FALSE,
  drop_levels       = TRUE,
         annotation_col    = mat_col,
         annotation_colors = mat_colors
         )

if(FALSE)
{
plotHeatmap(sce, features=top.markers, columns=order(sce$cluster), 
    #colour_columns_by=c("cluster"),
    colour_columns_by=c("Sample"),
    #cluster_cols=FALSE, center=TRUE, symmetric=TRUE, zlim=c(-5, 5)) 
    cluster_cols=TRUE, center=TRUE, symmetric=TRUE, zlim=c(-5, 5)) 
}
if(FALSE)
{
top.markers10 <- rownames(marker.set)[marker.set$Top <= 10]
plotHeatmap(nz.sce, features=top.markers10, columns=order(my.clusters),
    #colour_columns_by="cluster", cluster_cols=FALSE, 
    colour_columns_by="cluster", # cluster_cols=FALSE, 
    center=TRUE, symmetric=TRUE, zlim=c(-5, 5))
}
```

One can sort both the gene and sample dendrograms to improve the heatmap.

```{r dendsort}
library(dendsort)

mat <- tmpData
mat_cluster_cols <- hclust(dist(t(mat)))

sort_hclust <- function(...) as.hclust(dendsort(as.dendrogram(...)))

mat_cluster_cols <- sort_hclust(mat_cluster_cols)
#plot(mat_cluster_cols, main = "Sorted Dendrogram", xlab = "", sub = "")

mat_cluster_rows <- sort_hclust(hclust(dist(mat)))

pheatmap(tmpData,
           border_color      = NA,
  show_colnames     = FALSE,
  show_rownames     = FALSE,
  drop_levels       = TRUE,
         annotation_col    = mat_col,
         annotation_colors = mat_colors,
   cluster_cols      = mat_cluster_cols,
  cluster_rows      = mat_cluster_rows
         )
```

#### Challenges

Challenge? Compare t-SNE obtained here to that shown in the article and show expression level of reported markers genes on t-SNE plots.

Challenge? Identify genes that are upregulated in each cluster compared to others. 

<!--
"By setting direction="up", findMarkers will only return genes that are upregulated in each cluster compared to the others. This is convenient in highly heterogeneous populations to focus on genes that can immediately identify each cluster. While lack of expression may also be informative, it is less useful for positive identification."
-->

Challenge? Identify genes differentially expressed between a cluster and all others.

<!--
" findMarkers can also be directed to find genes that are DE between the chosen cluster and all other clusters. This should be done by setting pval.type="all", which defines the p-value for each gene as the maximum value across all pairwise comparisons involving the chosen cluster. Combined with direction="up", this can be used to identify unique markers for each cluster. However, this is sensitive to overclustering, as unique marker genes will no longer exist if a cluster is split into two smaller subclusters."

"It must be stressed that the (adjusted) p-values computed here cannot be properly interpreted as measures of significance. This is because the clusters have been empirically identified from the data. limma does not account for the uncertainty of clustering, which means that the p-values are much lower than they should be. This is not a concern in other analyses where the groups are pre-defined."
-->

Challenge? Identify genes whose distribution of expression, rather than their average expression, differs between clusters (Hint: overlapExprs() may help).

<!--
"The overlapExprs function may also be useful here, to prioritize candidates where there is clear separation between the distributions of expression values of different clusters. This differs from findMarkers, which is primarily concerned with the log-fold changes in average expression between clusters."
-->

Save object.

```{r}
rObjFile <- "Tcells_SCE_comb_session.RData"

# check file exists:
#tmpFileName <- file.path(inpDir, dataSubDir, rObjFile)
tmpFileName <- file.path(rObjFile)

# uncomment to save sesssion # save.image(file=tmpFileName)
```

## Other types of analyses beyond this brief introduction

Several tools for single cell analyses, eg Seurat, were not covered in this brief introduction this afternoon. Please refer to links above for more information on these and more advanced analyses such as progress along a differentiation pathway, or pseudotime, with monocle <!-- (Trapnell et al. 2014) --> or TSCAN <!-- (Ji and Ji 2016) and gene set enrichment analyses used for bulk data or designed single-cell methods like scde <!-- (Fan et al. 2016) -->. 


<!--
```{r knit}
#Â to knit thr Rmd into an html in R.
if(FALSE)
{
setwd("/home/participant/Course_Materials/SinglecellToUse/HumanBreastTCells/GRCh38/")
tmpFileName <- "crukSummerSchoolJuly2018_scRnaSeqCellPopId_practical.Rmd"
require(knitr) # required for knitting from rmd to md
require(markdown) # required for md to html 
knit("crukSummerSchoolJuly2018_scRnaSeqCellPopId_practical.Rmd", "crukSummerSchoolJuly2018_scRnaSeqCellPopId_practical.md") # creates md file
markdownToHTML("crukSummerSchoolJuly2018_scRnaSeqCellPopId_practical.md", "crukSummerSchoolJuly2018_scRnaSeqCellPopId_practical.html") # creates html file
}
```
-->

